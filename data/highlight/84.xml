<?xml version="1.0" encoding="UTF-8"?>
<publication marker="li ying">
    <journal>International Journal of Information Management</journal>
    <time>Volume 36, Issue 6, Part A, December 2016, Pages 963-975</time>
    <title>A survey on Test Suite Reduction frameworks and tools</title>
    <author>Saif Ur Rehman Khan.</author>Author links open the author workspace.Opens the author workspaceOpens the author workspacea. Numbers and letters correspond to the affiliation list. Click to expose these in author workspace <author>Sai Peck Lee.</author> Author links open the author workspace.Opens the author workspaceOpens the author workspacea. Numbers and letters correspond to the affiliation list. Click to expose these in author workspace <author>Raja Wasim Ahmad. </author>Author links open the author workspace.Opens the author workspaceb. Numbers and letters correspond to the affiliation list. Click to expose these in author workspace <author>Adnan Akhunzada.</author> Author links open the author workspace.Opens the author workspaceb. Numbers and letters correspond to the affiliation list. Click to expose these in author workspace <author>Victor Chang. </author>Author links open the author workspace.Opens the author workspacec. Numbers and letters correspond to the affiliation list. Click to expose these in author workspace
a
Department of Software Engineering, Faculty of Computer Science and Information Technology, University of Malaya, 50603 Kuala Lumpur, Malaysia
b
Department of Computer Science, COMSATS Institute of Information Technology (CIIT), Pakistan
c
International Business School Suzhou, Xi'an Jiaotong Liverpool University, Suzhou, China
Show more
    <doi>https://doi.org/10.1016/j.ijinfomgt.2016.05.025</doi>
Highlights
    <highlight id="1">An extensive review on automated support for TSR.</highlight>
    <highlight id="2">Presenting a thematic taxonomy to categorize the existing literature based on testing domains and corresponding parameters.</highlight>
    <highlight id="3">Providing a detailed comparative analysis of TSR frameworks based on the devised taxonometric parameters.</highlight>
    <highlight id="4">Synthesizing current state-of-the-art based on the underlying common philosophies.</highlight>
    <highlight id="5">Highlighting several potential research issues in this field of study.</highlight>
    <abstract> 
        <![CDATA[
Software testing is a widely accepted practice that ensures the quality of a System under Test (SUT). However, the gradual increase of the test suite size demands high portion of testing budget and time. Test Suite Reduction (TSR) is considered a potential approach to deal with the test suite size problem. Moreover, a complete automation support is highly recommended for software testing to adequately meet the challenges of a resource constrained testing environment. Several TSR frameworks and tools have been proposed to efficiently address the test-suite size problem. The main objective of the paper is to comprehensively review the state-of-the-art TSR frameworks to highlights their strengths and weaknesses. ]]><h target="2" match="part">Furthermore, the paper focuses on devising a detailed thematic taxonomy to classify existing literature that helps in understanding the underlying issues and proof of concept.</h><h target="3" match="part">Moreover, the paper investigates critical aspects and related features of TSR frameworks and tools based on a set of defined parameters.</h>  We also rigorously elaborated various testing domains and approaches followed by the extant TSR frameworks. The results reveal that majority of TSR frameworks focused on randomized unit testing, and a considerable number of frameworks lacks in supporting multi-objective optimization problems. Moreover, there is no generalized framework, effective for testing applications developed in any programming domain. Conversely, Integer Linear Programming (ILP) based TSR frameworks provide an optimal solution for multi-objective optimization problems and improve execution time by running multiple ILP in parallel.<h target="4" match="part"> The study concludes with new insights and provides an unbiased view of the state-of-the-art TSR frameworks.</h> Finally, <h target="5" match="part">we present potential research issues for further investigation to anticipate efficient TSR frameworks.</h></abstract>
    <keywords>Regression testing</keywords>
    <keywords>Test suite optimization</keywords>
    <keywords>Test Suite Reduction</keywords>
    <keywords>Frameworks</keywords>
    <keywords>Fault localization</keywords>
    <section name="Introduction" category="introduction">
        <![CDATA[
1. Introduction
Software testing ensures the quality and reliability of a System Under Test (SUT) by revealing maximum possible defects (Myers, Sandler, & Badgett, 2011). However, software testing is the most expensive quality assurance practice since it consumes up to 50% of the total software development cost (Ramler & Wolfmaier, 2006). Although, exhaustive testing (e.g., running all possible paths in the SUT (Lee & Chung, 2000)) is putative to provide high confidence to development organizations regarding the SUT quality (Kuhn & Okun, 2006). However, exhaustive testing is impractical due to time and budget constraints (Tassey, 2002). Moreover, execution of the entire test suite is also impractical, if high human interventions are required for testing a software system (Haug, Olsen, & Consolini, 2001). In the literature, researchers have reported different testing scenarios (Lin et al., 2012; Rothermel, Untch, Chu, & Harrold, 2001), which concludes that exhaustive testing requires a considerable amount of testing budget. Rothermel et al. (2001) reported that testing a product containing 20,000 lines of code requires seven weeks and several hundred thousand dollars to execute the entire test suite. Moreover, Lin et al. (2012) performed an empirical analysis of regression testing using 57,758 functions and 2320 test cases. They reported that the running time of a single test case ranges from 10 min to 100 min, which is based on the configuration sequence and required test activities. Finally, they estimated that to execute all 2320 test cases would require 36 test-bed days.
To deal with the aforementioned exhaustive testing problems, development organizations are attracted to adopt optimal testing strategies (Nachmanson, Veanes, Schulte, Tillmann, & Grieskamp, 2004). Analytics is useful in software design and testing (Chang, 2015b). Similarly, when we design a software, do not forget diverse security concerns (Akhunzada, Gani, Anuar et al., 2015; Akhunzada, Sookhak et al., 2015) to ensure that the software design supports penetration testing (ethical hacking) against hacking (Chang & Ramachandran, 2016).
In the literature, three main techniques have been discussed to support regression testing (Yoo & Harman, 2012): (i) test suite reduction: to find a reduced suite by permanently eliminating redundant test cases according to certain criteria, (ii) test case selection: to select such previously generated test cases that cover the modified portion of the software, and (iii) test case prioritization: to determine the ordering of test cases based on a particular objective such as to increase the rate of Fault Detection Effectiveness (FDE). In our context, the real challenge is to determine a subset of non-redundant test cases, which finds a maximum number of possible defects similar to the original test suite (Khan, Nadeem, & Awais, 2006). A tester can meet this challenge by randomly selecting the generated test cases (Chen, Kuo, Merkel, & Tse, 2010), but such random selection may end up as exclusion of essential test cases (Hao, Zhang, Wu, Mei, & Rothermel, 2012). Consequently, it has a negative impact on the fault detection capability of the reduced suite. A practical approach to solve the test-suite size problem is to find a minimal subset of test cases automatically, while keeping their fault detection capability similar to the original test suite. Test Suite Reduction (TSR) approach focuses on finding a minimal test suite by permanently discarding the redundant test cases from the original test suite (Dandan, Tiantian, Xiaohong, & Peijun, 2013). The optimal TSR problem is formally defined by Harrold, Gupta, and Soffa (1993) as stated below:
Given: A test suite, TS, and a set of test requirements R1, R2,…, Rn, that must be satisfied to provide the desired test coverage of the program, and subsets of TS, T1, T2,…, Tm, where each Ti is associated with each of the Ris such that any one of the test cases tcj of Ti can be used to test requirement Ri.

Problem: Find a Reduced Suite (RS) containing minimal test cases from TS that satisfies all test requirements Ri at least once.


The optimal TSR problem is known to be NP-complete problem and is equivalent to set cover problem (Michael & David, 1979). Researchers have recommended complete automation support for various activities of test suite development cycle, such as test generation, execution, and evaluation, to minimize high cost of regression testing. Bertolino (2007) has recommended 100% automated testing, which facilitates the tester to meet the challenges of a resource constrained testing environment. Motivated by this, researchers have proposed various frameworks that results various tools to provide automation facility during TSR process (Andrews, Haldar, Lei, & Li, 2006; Burger & Zeller, 2011; Campos, Riboira, Perez, & Abreu, 2012; Chae, Woo, Kim, Bae, & Kim, 2011; Dadeau, Ledru, & Du Bousquet, 2007; Horgan & London, 1992; Hsu & Orso, 2009; Jaygarl, Lu, & Chang, 2010; Kauffman & Kapfhammer, 2012; Li, Sahin, Clause, & Halfond, 2013; Pacheco & Ernst, 2007; Sampath, Sprenkle, Gibson, Pollock, & Greenwald, 2007; Sampath, Bryce, Jain, & Manchester, 2011; Wang, Ali, & Gotlieb, 2015; Woo, Chae, & Jang, 2007; Xie, Marinov, & Notkin, 2004; Xie, Zhao, Marinov, & Notkin, 2006; Zhang, Gu, Chen, Qi, & Chen, 2010; Zhang, Zhou, Hao, Zhang, & Mei, 2009). Consequently, automated TSR helps in accelerating the software delivery process compared to manual test filtering (Hsu & Orso, 2009). Automation has been achieved for Cloud storage for big data, which provide a supporting use case (Chang & Wills, 2016). Cloud service APIs Plays a vital role to integrate enterprise applications as they offer a set of protocols, which helps in connecting applications to various cloud services. APIs are useful for different disciplines such as business intelligence (Chang, 2014a) and social networks (Chang, 2015a, 2014b).
Prior works (Elberzhager, Rosbach, Münch, & Eschbach, 2012; Yoo & Harman, 2012) lack in considering and analyzing TSR frameworks that is necessary to understand the body of knowledge in the area of TSR. To the best of our knowledge, this is the first effort that studies the TSR frameworks and tools comprehensively. The main objective of this survey is to provide an up-to-date view and in-depth analysis of state-of-the-art that is necessary to understand the body of knowledge. The survey analyzes, synthesizes, and categorizes current state-of-the-art TSR frameworks and tools. Furthermore, the survey focuses on identifying future research opportunities in this field of study. The main contributions of the paper are listed below:
•
]]><h target="1" match="full">An extensive review on automated support for TSR.</h>

•
<h target="2" match="full">Presenting a thematic taxonomy to categorize the existing literature based on various testing domains, approaches, and their corresponding parameters.</h>

•
<h target="3" match="full">Providing a detailed comparative analysis of TSR frameworks based on the devised taxonometric parameters.

•
Highlighting the strengths and limitations of the TSR tools and frameworks related to a particular testing domain.</h>
<h target="4" match="full">Synthesizing current state-of-the-art based on the underlying common philosophies.</h>

•
<h target="5" match="full">Highlighting several potential research issues in this field of study.</h><![CDATA[


The rest of the paper is organized as follows: Section 2 presents a thematic taxonomy of TSR frameworks. Section 3 discusses current state-of-the-art TSR frameworks/tools based on various testing domains and approaches. Furthermore, it discusses strengths and weaknesses of existing TSR frameworks. Section 4 provides a comparison of current TSR frameworks based on the devised thematic taxonomy. Section 5 discusses potential research issues followed by concluding remarks in Section 6.]]></section>
    <section name=" Taxonomy of Test Suite Reduction (TSR) frameworks/tools;State-of-the-art Test Suite Reduction (TSR) frameworks" category="methods">
        <![CDATA[
2. Taxonomy of Test Suite Reduction (TSR) frameworks/tools
This section presents a taxonomy for the thematic classification of TSR frameworks and tools based on defined parameters as depicted clearly in Fig. 1. The defined parameters include: (i) approach type, (ii) testing paradigm, (iii) optimization type, (iv) coverage source, (v) execution platform, (vi) computational mode, (vii) license type, (viii) evaluation, (ix) customizability, and (x) support.
Fig. 1
Download high-res image (352KB)Download full-size image
Fig. 1. Taxonomy of the current state-of-the-art Test Suite Reduction (TSR) frameworks/tools.
The first identified parameter is the approach type, which represents the principal category of TSR approaches focused by a TSR framework. There are four main attributes for approach type: (i) coverage-based, (ii) search-based, (iii) Integer Linear Programming (ILP) based, and (iv) similarity-based. The coverage-based TSR approaches greedily select such test cases that cover maximum portions (e.g., statements or branches) of a program under test. In contrast, search-based approaches employ various search algorithms, such as Genetic Algorithm (Deb, Pratap, Agarwal, & Meyarivan, 2002), to find diverse test cases from the initial population. Conversely, ILP-based approaches determine minimal global solution for TSR problem based on the defined objectives and constraints (Black, Melachrinoudis, & Kaeli, 2004). Furthermore, ILP-based approaches achieve Pareto-optimal solution for multi-objective TSR problem (Baller, Lity, Lochau, & Schaefer, 2014). Notice that Pareto-optimal solution is a non-dominated solution, which cannot be further improved (Yoo & Harman, 2007). In contrast, similarity-based approaches focus on finding most different test cases based on the computed similarity degree between test cases (Coutinho, Cartaxo, & Machado, 2013). Note that similarity-based approaches heavily depend on a similarity matrix that shows the degree of similarity for all pairs of test cases. The similarity degree is calculated using a similarity measure such as Jaccard index (Jaccard, 1901).
The parameter testing paradigm represents the type of implementation language of the targeted SUT. The supported attributes are structured, object-oriented, or aspect-oriented. The other important parameter is the optimization type, which is regarded as a number of optimization objectives considered by a TSR framework. The optimization type is categorized into (i) single-objective: focuses either on finding a RS or computing the FDE, and (ii) multi-objective: focuses on both RS and FDE. In comparison to single-objective optimization, multi-objective supported tools need to find a best tradeoff between the targeted objectives and constraints (Wang et al., 2015).
The parameter coverage source is employed by the coverage supported frameworks to find a reduced test suite. The coverage source uses two main attributes: (i) source code and (ii) test execution profile. The frameworks that use the source code as a coverage source, determine the reduced suite by picking such test cases that cover maximum elements of the program source code. On the other hand, in the test execution profile-based category, first execution profiles are captured by running the entire test suite (Leon & Podgurski, 2003). Next, the supported framework determines the representative test cases based on the achieved coverage score of execution profiles.
The parameter execution platform represents the number of servers used by a test reduction framework to determine the RS. It can be broadly classified into: (i) single server and (ii) multiple servers. In the case of multiple servers, the TSR problem is solved by using divide-and-conquer strategy. Initially, the problem (large test suite) is divided into many sub-problems (several small test suites). Next, each sub-problem is executed on a single server with the aim to speed up the TSR process. The computational mode is the type of processing supported by a framework during TSR process and classified into two attributes: (i) online and (ii) offline. The online attribute accepts several smaller test suites in a sequential manner to determine the RS. In contrast, the offline attribute accepts the whole test suite to find an optimal solution.
The parameter license type is considered as a primary characteristic for the selection of TSR tools. This parameter defines three main categories including commercial, academic research, and free tools. License of Commercial tools can be acquired through payment. Academic research tools include prototypes and tools developed by research labs/groups, while free tools are available free of cost. These tools are mostly open source having common licenses such as GPL, ASL, EPL and CeCILL.
Evaluation parameter refers to external or internal evaluation of the selected frameworks. Internal evaluation disscusses those framework which have been evaluated by builders/design teams. While external evaluation refers to those framework which have been tested outside the environment where they have been originally built. Such external evaluation enhances the degree of confidence regarding usefulness of certain framework.
Customizability can be regarded as an ability of the tool to support desired alterations. This parameter defines three categories including Full, partial and No support for customizability. Fully customizable tools support major alterations (e.g., customizing major basic functions) while partial customizability allows only minor alterations (e.g., seamless integration into a given environment). For example, most of the open source tools support full customizability. On the other hand, most of the proprietary and commercial tools provide partial or no customizability.
Another significant characteristic of any tool is the support availability. Information about executables and written documentation is indispensable for a wider analysis of TSR tools. In our case, very few studies provided download links for executables and source code. So we have performed rigorous search to find the web links where tool support (executables, source code, written documentation) was actually available. We have approached personal web sites of authors and research groups in this regard. We have mainly divided available support into executables, written documentation (manuals, tutorials, instructions, examples etc.) and tool web site.
    
3. State-of-the-art Test Suite Reduction (TSR) frameworks
This section discusses current state-of-the-art TSR frameworks and also highlights their strengths and weaknesses. The effectiveness of TSR process heavily depends on the employed frameworks to support the test reduction process. Fig. 2 presents the timeline diagram of proposed TSR frameworks covering time period between year 1992 to year 2015.
Fig. 2
Download high-res image (452KB)Download full-size image
Fig. 2. Timeline diagram of TSR tools/frameworks.
Researchers have proposed a number of frameworks that support different programming languages, testing paradigms, and testing approaches. Existing TSR frameworks are categorized into five main classes based on the targeted testing domains and approaches: (i) randomized unit testing, (ii) user session testing, (iii) retargeted compilers testing, (iv) integer linear programming, and (v) automated fault-localization.

3.1. Randomized unit testing
Unit testing is one of the prominent software testing method that focuses on testing the smallest testable part of the SUT such as source code method, a group of methods, or classes along with related control data and operating procedures (Myers et al., 2011). Randomized unit testing refers to such testing mechanism, where some random elements are involved in selecting the parameters and/or methods (Beizer, 2002). Therefore, it generates distinct test inputs easily and quickly from common data structures, which can be useful in exposing defects (Andrews, Menzies, & Li, 2011). The following sub-sections debate on the existing randomized unit testing based TSR tools with special emphasis on their strengths and shortcomings.
3.1.1. Randomized unit testing based TSR tools
Majority of existing TSR tools focused on randomized unit testing, including ATAC (Horgan & London, 1992), Rostra (Xie et al., 2004), Raspect (Xie et al., 2006), RUTE-J (Andrews et al., 2006), Randoop (Pacheco & Ernst, 2007), GenRed (Jaygarl et al., 2010), Jtop (Zhang et al., 2009), TOBIAS (Dadeau et al., 2007), TEMSA (Wang et al., 2015) and Open-SourceRed (Kauffman & Kapfhammer, 2012). In the following, we briefly discuss current randomized unit testing focused TSR tools.
•
ATAC: Horgan and London (1992) proposed a unit testing based TSR tool called ATAC that eliminates redundant test cases based on the high coverage of selected data-flow coverage metric. The main advantage of ATAC is to support the test evaluation process by implicitly facilitating in obtaining execution slices of program paths. However, ATAC lacks in detecting a significant number of crucial defects, since it only supports selective testing.
•
Rostra: Xie et al. (2004) presented a formal object-oriented unit testing based framework, called Rostra that facilitates to evaluate a test suite’s quality by finding similar unit test cases based on the equivalent objects. Although, it is argued that Rostra efficiently generates test sequences such as violating and satisfying with few ineffective test sequences. However, Rostra uses state-space exploration, which is heuristically pruned. Ultimately, the proposed tool never guarantees to determine minimal solution.
•
Raspect: Xie et al. (2006) extended Rostra (Xie et al., 2004) to detect the redundant unit test cases for aspect-oriented programs, called Raspect that preserves structural coverage using similar states for the considered objects. The proposed tool automatically detects similar test cases, which do not exercise the new behavior of the program under test. Consequently, it requires less time for manual inspection of the computed solution. However, Raspect performance degrades significantly in the case of non-availability of program’s algebraic specification, which is used for asserting the program behavior.
•
RUTE-J: Andrews et al. (2006) proposed a tool, called RUTE-J to test a small portion of the program under test such as methods or classes. It uses delta debugging technique (Burger & Zeller, 2011) to isolate the failure-inducing inputs of the program under test. Furthermore, it effectively manages test’s storage and retrieval operations. However, RUTE-J tool interaction experience is highly required as the tester has to enter some technical inputs.
•
Randoop: Pacheco and Ernst (2007) presented a deterministic execution feedback-directed tool, called Randoop that determines and eliminates the test inputs that either throw exceptions or create similar objects. However, Randoop lacks in creating a parameter graph instead it used previously generated test sequences of a component set, which are useful to create similar objects or throw exceptions. Consequently, Randoop determines a small number of redundant test cases.
•
GenRed: Jaygarl et al. (2010) proposed a new feedback-directed randomized tool, called GenRed that discards generated redundant method sequences. The authors empirically evaluated the performance of GenRed and Randoop (Pacheco & Ernst, 2007) using a total testing time criterion. Although, GenRed showed good performance compared to Randoop in terms of size of the RS and achieved code coverage, however, the authors did not report required time mechanism necessary to understand the core mechanism of controlling GenRed working.
•
Jtop: Zhang et al. (2009) focused on managing JUnit test cases and proposed an Eclipse IDE plug-in, called Jtop, which does not require any code coverage information for TSR. The authors define and substitute ‘relevant relation’ notion with traditional coverage-based information for TSR. Consequently, it enables Jtop to determine a set of related test cases covering particular elements of the program under test. Although, authors evaluated the performance of Jtop using a thorough set of test data, but they did not discuss about the achieved results.
•
TOBIAS: Dadeau et al. (2007) presented a semi-automated combinatorial testing based tool, called TOBIAS. The proposed tool captures the test engineers knowledge to write test patterns, which requires significant manual effort. After that, test patterns are unfolded that might results into a set of millions of abstract and redundant test cases. Therefore, to better cope with the impact of combinatorial unfolding, TOBIAS employs a generic filtering and selection mechanisms.
•
TEMSA: Wang et al. (2015) presented a tool called TEst Minimization using Search Algorithms (TEMSA). The proposed tool supports efficient software product line testing using feature pairwise coverage criterion. The test engineer needs to select the test minimization objective with highest importance. Then, TEMSA recommends most suitable weight-based search algorithm based on a given importance value of the test minimization objective. After that, test engineer might start running the test minimization process. Finally, TEMSA generates the minimized test suite.
•
Open-SourceRed: Kauffman and Kapfhammer (2012) proposed an open-source framework supported by a prototype tool, called Open-SourceRed, which contains two open-source components: (i) Proteja and (ii) Modificare. Proteja accepts two main inputs including, Java Program and JUnit test suite, and generates a test-coverage report in a binary matrix format based on statement, method, and class coverage criteria. In contrast, Modificare takes test-coverage and timing information to perform TSR and also produces a modified suite file.

3.1.2. Comparison of randomized unit testing based TSR tools
A significant number of existing TSR tools (53%, 10/19) focused on randomized unit testing. However, there are two key limitations associated with current tools in the category of randomized unit testing that need further research to design more effective tools. The limitations are: (i) test oracle problem, which demands manual effort to evaluate the test results, and (ii) generating infeasible test cases, since execution does not represent the actual scenario. Currently, tools are focusing on different programming domains, including structured (Horgan & London, 1992), object-oriented (Andrews et al., 2006; Dadeau et al., 2007; Jaygarl et al., 2010; Kauffman & Kapfhammer, 2012; Xie et al., 2006; Zhang et al., 2009), and aspect-oriented (Xie et al., 2004). However, there is no generalized tool, which is applicable and effective to test programs developed using various programming domains.
A variety of tools for randomized unit testing has been proposed, where the majority was based on code coverage information of the program under test to determine the reduced suite (Andrews et al., 2006; Dadeau et al., 2007; Horgan & London, 1992; Jaygarl et al., 2010; Pacheco & Ernst, 2007; Xie et al., 2004, 2006). However, there are a number of drawbacks of using code coverage information: (i) instrumented version of source code needs to be run to collect coverage information, (ii) coverage storage and management cost proportionally increases with respect to the program size, and (iii) previously collected coverage information becomes inconsistent due to software evolution. Consequently, current coverage-based TSR tools might be difficult to test real-world applications due to extra cost in terms of coverage collection, storage, and management. In this situation, compared to ATAC (Horgan & London, 1992), Rostra (Xie et al., 2004), Raspect (Xie et al., 2006), RUTE-J (Andrews et al., 2006), Randoop (Pacheco & Ernst, 2007), GenRed (Jaygarl et al., 2010), TOBIAS (Dadeau et al., 2007), TEMSA (Wang et al., 2015) and Open-SourceRed (Kauffman & Kapfhammer, 2012), Jtop (Zhang et al., 2009) is an effective tool, since it does not require any code coverage information. Furthermore, Jtop automatically extracts static call graph to determine a test case covering particular program elements; however, it usually contains over approximations. In other words, over approximation results significant call relationships, which would never occur in real program execution.
In contrast to other randomized unit testing tools, both Rostra (Xie et al., 2004) and Raspect (Xie et al., 2006) dynamically monitor the test executions to detect and remove the redundant test cases based on equivalent object’s states. To achieve this, they used bounded exhaustive generation with state-space exploration. Consequently, they required less time for test synthesis as redundant test cases are automatically detected. However, both Rostra (Xie et al., 2004) and Raspect (Xie et al., 2006) requires significant resources in terms of memory to keep track of all object’s states.
Among all, two TSR tools including, Randoop (Pacheco & Ernst, 2007) and GenRed (Jaygarl et al., 2010), are based on feedback-directed mechanism, which enables a limited access to the objects. For instance, if Randoop (Pacheco & Ernst, 2007) is unable to locate an object of the correct type required to invoke a method in the existing set of sequences, then it will never be able to invoke such method. Moreover, Randoop (Pacheco & Ernst, 2007) is easy to use but lacks in supporting multi-threaded programs. Ultimately, Randoop (Pacheco & Ernst, 2007) is unable to concurrently execute the methods. Thus, in this situation, GenRed (Jaygarl et al., 2010) outperformed Randoop (Pacheco & Ernst, 2007) by using on-demand generation to create necessary test inputs. Furthermore, GenRed (Jaygarl et al., 2010) assigns high priority to the methods with lower coverage to invoke method in the current set of method sequences.
Some randomized unit testing supported TSR tools including, ATAC (Horgan & London, 1992), RUTE-J (Andrews et al., 2006), Randoop (Pacheco & Ernst, 2007), GenRed (Jaygarl et al., 2010), TOBIAS (Dadeau et al., 2007), and Open-SourceRed (Kauffman & Kapfhammer, 2012), require high human interventions during TSR process for their correct functioning, which ultimately needs additional testing time. For instance, ATAC (Horgan & London, 1992) requires high human interaction to create test data, test script, and evaluate test results. Similarly, TOBIAS (Dadeau et al., 2007) requires significant effort of test engineer to write test patterns based on their knowledge. In this situation, Raspect (Xie et al., 2006) requires less manual inspection, since it automatically detects redundant test cases using program algebraic specifications. In contrast, some tools such as RUTE-J (Andrews et al., 2006), Randoop (Pacheco & Ernst, 2007), GenRed (Jaygarl et al., 2010), and Open-SourceRed (Kauffman & Kapfhammer, 2012) require high user experience regarding tool interaction in order to enter some technical inputs. For example, RUTE-J (Andrews et al., 2006) heavily depends on user interaction experience as it has no intelligence to automatically check the correctness of the run-time entered values, including method parameters range and weighting scheme. Similarly, Randoop (Pacheco & Ernst, 2007) requires the test engineer’s interaction to set the time limits for determining the RS. In this situation, Jtop (Zhang et al., 2009) outperformed other proposed tools since it does not require any user experience. Jtop automatically extracts the static call graph in order to generate non-redundant test cases. Future tools can further improve the capability of Jtop by constructing a dynamic call graph (Lee et al., 2007), which extracts an exact record of program execution frequencies of call relationship based on the profile information. Ultimately, it is beneficial in optimizing overall program under test behavior. However, some cost may be required in terms of non-invasive capture tools to adequately capture the execution traces for generating profile information.
3.2. User session testing
Web applications have been widely accepted as a cost-effective communication medium for business organizations over past few years. Due to complexities of web applications, including frequent user interaction and heterogeneous components, traditional testing techniques and theories cannot be directly used to ensure the quality of web applications (Li & Xing, 2011). User session testing is increasingly adopted for web application’s testing (Di Lucca & Fasolino, 2006). It mainly focuses on selecting user session data such as sequence of user actions with the web application, which are recorded in web server logs (Akhunzada, Gani, Hussain, & Khan, 2015a; Elbaum, Rothermel, Karre, & Fisher, 2005). Collected user session data may contain significant redundancy, since common scenarios of application execution are achieved by the users.
3.2.1. User session testing based TSR tools
To the best of our knowledge, researchers have proposed two TSR tools namely, USbRed (Sampath et al., 2007) and CPUT (Sampath et al., 2011), to support user session testing for web applications by determining and eliminating the duplicate user session data.
•
USbRed: Sampath et al. (2007) proposed a framework supported by USbRed tool to incrementally reduce a set of recorded user session data. It mainly focuses on maintaining coverage and fault detection capabilities in terms of all base requests and use case representation. The authors applied concept analysis to determine the minimal test suite by reducing a set of user sessions. Concept analysis is a clustering technique that groups the common user sessions based on similar distinct attributes (Di Lucca & Fasolino, 2006). The authors reported that USbRed along with three proposed TSR heuristics effectively reduces the user- session test data along with high coverage and fault detection of base requests.
•
CPUT: Sampath et al. (2011) presented a general tool designed to test web applications, called CPUT that supports the user by easily collecting and reducing the user-session based test cases. The authors conducted an experimental study to show the efficacy of CPUT using a small-size web application and reported that the tool achieved nearly 20% of TSR. However, they did not mention the achieved fault-detection rate.

3.2.2. Comparison of user session testing based TSR tools
User session testing based TSR tools, including USbRed (Sampath et al., 2007) and CPUT (Sampath et al., 2011), offers an effective and cheap mechanism for selecting representative user session data by capturing execution traces as created by real users. However, they need non-invasive capture tools to adequately capture the execution traces. On the other hand, they provide an external (or black-box) point of view, which is cost-effective in comparison to code coverage-based techniques and useful to expose functionality-related faults. Furthermore, proposed tools significantly decreased the testing cost in terms of finding the inputs as they generate a large pool of test cases without analyzing the implementation details. However, they achieved limited code coverage as specific input is required to exercise certain program paths. Ultimately, they never ensure exercising all program paths.
USbRed (Sampath et al., 2007) generates a large session data covering commonly used scenarios only, but incapable to explore the user session data in a systematic manner. Consequently, it lacks in covering rarest scenarios, which are useful in detecting critical faults. In contrast, CPUT (Sampath et al., 2011) ensures t-way such as 2-way combinatorial coverage of inter-window interactions to select a rare pair, which is effective to determine different faults using common execution traces. Combinatorial explosion is one main issue that can negatively affect the performance of CPUT (Sampath et al., 2011). Clustering of similar user sessions may be a viable solution to avoid the combinatorial explosion. On the other hand, to capture HTTP request along with related data, CPUT (Sampath et al., 2011) is supported by a generalized logger, which can be easily deployed on publicly available Apache server running on Linux or Windows platform. Conversely, USbRed (Sampath et al., 2007) is supported by an uncommon Resin web server. Both USbRed (Sampath et al., 2007) and CPUT (Sampath et al., 2011) use a different format of test cases. USbRed (Sampath et al., 2007) uses ordered HTTP requests as a test case. While, CPUT (Sampath et al., 2011) uses XML format based test cases, which can be easily parsed and processed by open-source XML parser and replay tools, respectively.
There are two main issues of USbRed (Sampath et al., 2007) and CPUT (Sampath et al., 2011) tools: (i) captured user session data becomes invalid due to small modification in web applications (e.g., change in page name, links, and options) and (ii) generated all user session data cannot be executed due to time constraints. Future tools should need to consider most recent user sessions to solve the test-suite size problem by finding minimal test inputs. Moreover, future tools can employ genetic algorithm (Harman & Jones, 2001) or software agent-based technology (Jennings, 2000), which has the high potential to provide efficient and effective solution for testing dynamic behavior of web applications.
3.3. Retargeted compilers testing
Processors need to be redesigned to provide better solutions for embedded software, which consequently require constructing new compilers for redesigned processors (Woo et al., 2007). The retargeting compiler is an efficient approach that supports the development of a new compiler for a processor by reusing and adjusting the existing compilers rather than building it from scratch (Boujarwah & Saleh, 1997). Generally, a large number of test cases are generated using source code by employing source-language grammar coverage criteria. Consequently, it demands automation support to reduce the test-suite size.
3.3.1. Retargeted compilers testing based TSR tools
To the best of our knowledge, researchers have proposed two tools, including RTL (Woo et al., 2007) and PLOOSE (Chae et al., 2011) that focus on testing the back-end of a retargeted compiler for reducing the test suite, since the back-end relies on the targeted processor.
•
RTL: Woo et al. (2007) proposed a high-level abstraction based TSR framework for retargeted compilers by using the intermediate representations of test input programs. Notice that intermediate representation is coded in Register Transfer Language (RTL). The proposed framework consists of two tools, namely Test Generator and Test Filter, to filter test cases based on the RTL rule coverage criterion. The authors experimentally evaluated the effectiveness of C-covering and RTL-covering test suites. They concluded that RTL achieved a significant test-size reduction for the considered study.
•
PLOOSE: Chae et al. (2011) extended RTL-based framework by proposing a new tool, called PLOOSE that determines minimal tests based on intermediate representation of test inputs. It contains two main components: (i) Test Suite Generator, generates a number of test cases based on the given C grammar-coverage criteria, and (ii) Test Suite Reducer, converts compatible tests into RTL code using a translator, then finds and eliminates similar tests using RTL coverage. The authors conducted an empirical study and reported that PLOOSE achieved over 90% on average test-size reduction.

3.3.2. Comparison of retargeted compiler testing based TSR tools
RTL (Woo et al., 2007) and PLOOSE (Chae et al., 2011) primarily focused on testing the back-end of a retargeted compiler using intermediate representation to determine the RS. Researchers have mentioned two main advantageous of intermediate code-based testing for retargeted compilers: (i) determining more efficient test cases compared to the traditional code-based testing and (ii) adequately testing retargeted compiler in case of time limitations (Chae et al., 2011). RTL determines the RS based on grammar-based test generation approaches and retains redundant test cases in initial test optimization stage. However, RTL requires significant time in terms of test generation, which ultimately do not scale for large-size embedded applications. Future tools can enhance the performance of RTL by avoiding such redundant test cases, which would ultimately need to be eliminated. In contrast, PLOOSE considers additional grammar-coverage criteria to generate and minimize test suites. However, its Test Suite Reducer component needs to be enhanced in order to support new TSR techniques and intermediate languages.
3.4. Integer linear programming
All previously discussed tools except TEMSA (Wang et al., 2015) focused on solving single-objective TSR problem (i.e. to reduce the test-suite size by maintaining high coverage using certain criteria of the SUT), which consequently produces a sub-optimal solution. However, more than one objective such as minimal test execution time and maximal fault-detection is required to determine an optimal solution. Integer Linear Programming (ILP) is commonly used technique to determine the possible “Best” solution for a defined mathematical model for a set of objectives and constraints (Akhunzada, Gani, Hussain, & Khan, 2015b; Williams, 2013).
3.4.1. Integer linear programming based TSR tools
To the best of our knowledge, researchers have proposed two tools namely, MINTS (Hsu & Orso, 2009) and EDTSO (Li et al., 2013), which are based on encoding TSR problem as an ILP problem.
•
MINTS: Hsu and Orso (2009) proposed a framework to support a wide range of TSR problems by handling various objectives using ILP-based approach. The proposed framework is supported by a tool, called MINTS that provides the flexibility of specifying multi-objective TSR problems. Based on the defined encoding mechanism, related objectives are given directly to one or more supporting ILP solvers. The proposed tool is freely available and due to its modular structure it can be easily plug-in different ILP solvers. The authors experimentally evaluated the performance of the tool and concluded that MINTS is effective and practical to test-suite size problem.
•
EDTSO: Li et al. (2013) proposed a novel approach in order to optimize the energy usage of test suites. The proposed approach is supported by a prototype tool, called EDTSO, which focuses on an energy-efficient reduced test suite. EDTSO is effective for testing Android applications having limited energy budget by automatically producing energy-efficient RS. The authors performed a pair-wise comparison of the energy usage of the RS, including variability, potential impact, and effectiveness as determined by the proposed approach and traditional approach. They reported that EDTSO maintained coverage along with minimal energy consumption ranging from 5% to 48% compared to traditional TSR techniques. Later, Li, Jin, Sahin, Clause, and Halfond (2014) extended EDTSO by integrating it with existing test workflows and also by utilizing other resource constraints such as test execution time.

3.4.2. Comparison of integer linear programming based TSR tools
Integer Linear Programming (ILP) based TSR tools, including MINTS (Hsu & Orso, 2009) and EDTSO (Li et al., 2013), determined an optimal solution using multiple ILP solvers by considering defined objectives and constraints. In addition, improved execution time is achieved using a wide range of ILP solvers by running multiple solvers in parallel fashion and then simplifying the generated solutions in a sequential manner. MINTS provides any set of test-related data according to the need of a testing environment. Appropriate selection of test data greatly affects the outcome of MINTS, which heavily depends on the tester’s expertise. Furthermore, non-availability of historical testing information makes MINTS impractical to compute the optimal solution. Moreover, both MINTS and EDTSO need to execute each test case in order to collect profiles such as time or memory complexity and test-code structural coverage by instrumenting the targeted program. Certainly, high execution cost is required for instrumentation, which might be a key concern while employing these TSR tools.
3.5. Automated fault-localization
Fault-localization (also known as fault isolation or root cause analysis) is the most expensive and time consuming activity in debugging (Gong, Su, Wang, Ma, & Yu, 2015). It focuses on finding the exact location of SUT’s faults to improve the fault-detection process (Digiuseppe & Jones, 2014).
3.5.1. Automated fault-localization based TSR tools
To the best of our knowledge, researchers have proposed three tools including SrTC (Zhang et al., 2010), JINSI (Burger & Zeller, 2011) and GZoltar (Campos et al., 2012), to improve fault-localization effectiveness with the distribution uniformity of test cases.
•
SrTC: Zhang et al. (2010) presented very first time the concept of relative redundancy for TSR and focused to balance the uneven distribution in the RS. The authors proposed a framework supported by SrTC tool, which intentionally retained a small number of redundant test cases to improve fault-localization effectiveness. SrTC contains three modules: (i) Reduction Processor, (ii) w Processor, and (iii) Evaluation, which uses and produces five types of data: (1) T, (2) REP, (3) TIE, (4) CAN, and (5) REL-REP. To judge the efficacy of SrTC, the authors conducted an experimental study by using NanoXM (Do, Elbaum, & Rothermel, 2004) as a subject program. According to the reported results, the average value of reduction was 24.25%. However, the fault-localization effectiveness was improved significantly.
•
JINSI: Burger and Zeller (2011) proposed a JINSI tool that uses passing and failing execution information in order to determine faulty program entities. First of all, JINSI records and replays interactions of a given objects set and effectively reproduces the failure inducing method calls. After that, JINSI filters the sequence of failure inducing method calls by combining event slicing and delta debugging techniques, which result in a minimized object interaction. Eventually, the minimal set of unit tests is generated using the reduced interaction information. The authors experimentally evaluated the performance of the JINSI tool by applying it on six different Java subject programs. Finally, they reported that JINSI achieved an average of 0.22% search space reduction of the original Java program and produced a test driver containing eight to twelve failure-reproducing interactions.
•
GZoltar: Campos et al. (2012) presented a GZoltar toolset that is a spectrum-based fault localization plug-in for the Eclipse Integrated Development Environment (IDE). The proposed tool supports a constraint-based approach (Campos, 2012) to perform TSR, while ensuring the same code coverage compared to the original test suite. Moreover, GZoltar facilitate the user to prioritize the RS based on the test execution time and cardinality. It also automatically produces different visual representations of the diagnostic report, which ultimately facilitate the users in detecting the most suspicious parts of the program’s source code. As a result, the fault localization process can be improved using minimal possible time. GZoltar provides the infrastructure to instruments the source code of the SUT in order to automatically generate runtime data. Next, the test cases are executed to collect the code coverage data and execution traces. Finally, the collected coverage information is passed to constraint solver in order to determine minimal RS.

3.5.2. Comparison of automated fault-localization based TSR tools
The researchers have concluded that faults usually reside in execution paths of failed test cases (Dandan et al., 2013). Thus, the statements in failed test cases’ paths might be more beneficial in improving fault-localization effectiveness. However, SrTC (Zhang et al., 2010) only consider the coverage information of test cases instead of concrete path information. Consequently, SrTC (Zhang et al., 2010) may remove the test cases which are relevant to fault-localization requirements and ultimately achieves less fault-localization effectiveness. In contrast, JINSI (Burger & Zeller, 2011) performs better in terms of improving fault-localization effectiveness, since it minimizes failing test cases based on call/return traces. Ultimately, JINSI (Burger & Zeller, 2011) has the high potential to determine the significant number of faults than SrTC (Zhang et al., 2010). In comparison to SrTC (Zhang et al., 2010) and JINSI (Burger & Zeller, 2011), GZoltar (Campos et al., 2012) can speed-up the fault-localization process, since it provides different visual representations of the generated diagnostic report.]]></section>
    <section name="Thematic taxonomy based comparison of Test Suite Reduction (TSR) tools;Open research issues" category="results">
        <![CDATA[
4. Thematic taxonomy based comparison of Test Suite Reduction (TSR) tools
This section compares existing Test Suite Reduction (TSR) tools based on the devised thematic taxonomy (presented in Section 2) to highlight the commonalities and differences among the reported tools (depicted in Tables 1 and 2). We critically analyzed current TSR tools based on several parameters: (i) approach type, (ii) testing paradigm, (iii) optimization type, (iv) coverage source, (v) execution platform, (vi) computational mode, (vii) license type, (viii) evaluation, (ix) customizability, and (x) support.
Table 1. Taxonomy based comparison of TSR frameworks/tools.
Tool Name	Approach Type	Testing Paradigm	Optimization Type	Coverage Source	Execution Platform	Computational Mode	License
ATAC (Horgan & London, 1992)	Coverage-based	Structured	Single-objective	Source Code	Single Server	Offline	Free
Rostra (Xie et al., 2004)	Coverage-based	Object-oriented	Single-objective	Execution Profile	Single Server	Offline	Research
Raspect (Xie et al., 2006)	Coverage-based	Aspect-oriented	Single-objective	Execution Profile	Single Server	Offline	Research
RUTE-J (Andrews et al., 2006)	Coverage-based	Object-oriented	Single-objective	Source Code	Single Server	Offline	Free
Randoop (Pacheco & Ernst, 2007)	Coverage-based	Object-oriented	Single-objective	Source Code	Single Server	Offline	Free
GenRed (Jaygarl et al., 2010)	Coverage-based	Object-oriented	Single-objective	Source Code	Single Server	Offline	Research
Jtop (Zhang et al., 2009)	–	Object-oriented	Single-objective	–	Single Server	Offline	Free
TOBIAS (Dadeau et al., 2007)	Coverage-based	Object-oriented	Single-objective	Source Code	Single Server	Offline	Research
TEMSA (Wang et al., 2015)	Search-based	Object-oriented	Multi-objective	Feature Model	Single Server	Offline	Research
Open-SourceRed (Kauffman & Kapfhammer, 2012)	Coverage-based, Search-based	Object-oriented	Single-objective	Source Code	Single Server	Offline	Free
USbRed (Sampath et al., 2007)	Similarity-based	Object-oriented	Single-objective	Source Code	Single Server	Online	Research
CPUT (Sampath et al., 2011)	Coverage-based	Object-oriented	Single-objective	Source Code	Single Server	Online	Research
RTL (Woo et al., 2007)	Coverage-based	Structured	Single-objective	Source Code	Single Server	Offline	Research
PLOOSE (Chae et al., 2011)	Coverage-based	Structured	Single-objective	Source Code	Single Server	Offline	Research
MINTS (Hsu & Orso, 2009)	ILP-based	Object-oriented	Multi-objective	Source Code	Multiple Server	Offline	Free
EDTSO (Li et al., 2013)	ILP-based	Object-oriented	Multi-objective	Source Code	Multiple Server	Offline	Research
SrTC (Zhang et al., 2010)	Coverage-based	Object-oriented	Single-objective	Source Code	Single Server	Offline	Research
JINSI (Burger & Zeller, 2011)	Coverage-based	Object-oriented	Single-objective	Source Code	Single Server	Offline	Research
GZoltar (Campos et al., 2012)	Coverage-based	Object-oriented	Single-objective	Source Code	Single Server	Offline	Research/Commercial
Table 2. Taxonomy based Comparison of TSR Frameworks/Tools.
Tool Name	Evaluation	Customizability	Support
Internal	External	Full	Partial	Written Documentation	Executable	Web Site	Web link (Source Code, exe files, packages, Help and documentation)
ATAC (Horgan & London, 1992)	✓	✓	✓	×	✓	✓	✓	http://invisible-island.net/atac/atac.html
Rostra (Xie et al., 2004)	✓	✓	×	×	×	×	×	http://taoxie.cs.illinois.edu/research.htm
Raspect (Xie et al., 2006)	✓	×	×	×	×	×	×	http://taoxie.cs.illinois.edu/publications.htm
RUTE-J (Andrews et al., 2006)	✓	✓	✓	×	✓	✓	✓	http://staff.unak.is/andy/MScTesting0708/Assignments/RUTEJstuff/rutej-1.2.zip
http://staff.unak.is/andy/MScTesting0708/assignments.htm
Randoop (Pacheco & Ernst, 2007)	✓	✓	✓	×	✓	✓	✓	http://mernst.github.io/randoop/
http://randoop.codeplex.com/

GenRed (Jaygarl et al., 2010)	✓	×	×	×	×	×	×	N/A
Jtop (Zhang et al., 2009)	✓	✓	✓	×	✓	✓	✓	https://code.google.com/p/pku-jtop/
TOBIAS (Dadeau et al., 2007)	✓	✓	N/A	N/A	✓	N/A	✓	http://www.irisa.fr/en/cote Supporting information in French
TEMSA (Wang et al., 2015)	✓	×	×	×	✓	✓	✓	http://zen-tools.com/TEMSA/
Open-SourceRed (Kauffman & Kapfhammer, 2012)	✓	×	✓	×	✓	✓	✓	https://github.com/kauffmj/modificare
USbRed (Sampath et al., 2007)	✓	×	×	×	×	×	✓	http://hiper.cis.udel.edu/doku.php/projects/webapps
http://hiper.cis.udel.edu/udsacl/doku.php/research/home (Pvt research space)

CPUT (Sampath et al., 2011)	✓	×	×	×	×	×	✓	http://userpages.umbc.edu/∼sampath/CPUT_Web.html
RTL (Woo et al., 2007)	✓	×	N/A	N/A	N/A	N/A	×	http://oos.cse.pusan.ac.kr Supporting information in Korean
PLOOSE (Chae et al., 2011)	✓	×	N/A	N/A	N/A	N/A	✓	http://oos.cse.pusan.ac.kr/ploose/ Supporting information in Korean
MINTS (Hsu & Orso, 2009)	✓	✓	×	✓	✓	✓	✓	http://www.cc.gatech.edu/∼orso/software/mints/
http://www.cc.gatech.edu/∼orso/software/mints/mints.tgz

EDTSO (Li et al., 2013)	✓	×	×	×	×	×	×	http://www-scf.usc.edu/∼dingli/
SrTC (Zhang et al., 2010)	✓	×	N/A	N/A	N/A	N/A	N/A	N/A
JINSI (Burger and Zeller, 2011)	✓	✓	×	✓	✓	✓	×	https://www.st.cs.uni-saarland.de/dd/
GZoltar (Campos et al., 2012)	✓	✓	×	✓	✓	✓	✓	http://www.gzoltar.com/
http://www.gzoltar.com/lib/
Current state-of-the-art TSR tools can be classified into coverage-based, search-based, similarity-based, and Integer Linear Programming (ILP) based approaches. Majority of the existing tools (Andrews et al., 2006; Burger & Zeller, 2011; Campos et al., 2012; Chae et al., 2011; Dadeau et al., 2007; Horgan & London, 1992; Jaygarl et al., 2010; Kauffman & Kapfhammer, 2012; Pacheco & Ernst, 2007; Sampath et al., 2011; Woo et al., 2007; Xie et al., 2004, 2006; Zhang et al., 2010) consider coverage of the SUT as a base to determine the reduced suite. However, Open-SourceRed (Kauffman and Kapfhammer, 2012) and TEMSA (Wang et al., 2015) follows search-based techniques to find the diverse subset of a test suite. Similarly, MINTS (Hsu and Orso, 2009) and EDTSO (Li et al., 2013) employ ILP-based approach to determine the global optimal solution for TSR problem. In contrast, USbRed (Sampath et al., 2007) employs similarity-based technique to find the most different test cases from a test suite. In comparison to search-based approaches, coverage-based approaches require extra computational time to determine the coverage of a program using certain coverage criteria such as statement, branch, or path coverage. Moreover, coverage-based approaches may produce optimal or near-optimal solution using greedy algorithms (Khan, Peck, Parizi, & Elahi, 2013). In contrast, search-based approaches employ different types of search algorithms, such as Genetic Algorithm, to determine a global optimal solution based on the defined fitness function. However, search-based approaches can be very time-consuming especially if the fitness function is evaluated in terms of increased coverage provided by the different possible solutions.
The attributes of testing paradigm parameters state the type of programming languages in which the targeted SUT is developed. The TSR tools are broadly classified into structured, object-oriented, and aspect-oriented implementation platforms. The tools in Chae et al. (2011), Horgan and London (1992), Woo et al. (2007); Andrews et al. (2006), Burger and Zeller (2011), Hsu and Orso (2009), Jaygarl et al. (2010), Kauffman and Kapfhammer (2012), Li et al. (2013), Pacheco and Ernst (2007), Sampath et al. (2007, 2011), Xie et al. (2004), Zhang et al. (2010, 2009); and (Xie et al., 2006), consider structured, object-oriented, and aspect-oriented programming platforms, to minimize the test-suite size, respectively. The TSR tools can be categorized into single-objective and multi-objective tools. The single-objective based optimization type focuses on cost such as loss in fault-detection capability or effectiveness in terms of size of the RS. Alternatively, MINTS (Hsu and Orso, 2009) and EDTSO (Li et al., 2013) focus on the multi-objective optimization type using ILP-based approach, which attempts to balance the tradeoffs between both cost and effectiveness measures. Multi-objective is preferable to the tester as it generates different possible solutions of considered objectives and constraints, which consequently helps the tester in the decision-making process. However, in comparison, single-objective takes less computation time, since it only considers either cost or effectiveness measure.
The coverage source provides a base to determine the representative test cases. The tools in Andrews et al. (2006), Burger and Zeller (2011), Horgan and London (1992), Jaygarl et al. (2010), Kauffman and Kapfhammer (2012), Pacheco and Ernst (2007), Sampath et al. (2007, 2011), Woo et al. (2007), Zhang et al. (2010, 2009) and Xie et al. (2004, 2006) consider source code and test execution profile, respectively. In comparison to source code, test execution profiles are expensive as they can be collected by executing the entire test suite prior to initiating the test reduction process. The execution platform considers a single server and multiple server attributes to support the reduction process. Among others, MINTS (Hsu and Orso, 2009) and EDTSO (Li et al., 2013) use multiple servers attribute to concurrently solve the TSR problem. As a result, the computational time taken by MINTS (Hsu and Orso, 2009) and EDTSO (Li et al., 2013) significantly reduced at the cost of extra hardware resources. The computational mode exploits offline (Andrews et al., 2006; Burger & Zeller, 2011; Horgan & London, 1992; Jaygarl et al., 2010; Kauffman & Kapfhammer, 2012; Pacheco & Ernst, 2007; Woo et al., 2007; Xie et al., 2004, 2006; Zhang et al., 2010, 2009) and online (Sampath et al., 2007, 2011) attributes to generate the representative solution. In comparison to offline, online computational mode supports real-time testing, especially in the case of web-based applications.
The attributes of license type parameters define whether the TSR tool is freely available or a commercial product. The tools in (Horgan and London, 1992; Woo et al., 2007) and (Sampath et al., 2011) are available as free and proprietary, respectively. Analysis of TSR tools reveals that most of the existing tools are products of multiple academic research projects. While commercial support for such tools is rather scarce. In contrast, most of the freely available open source tools provide full customizability (ATAC (Horgan & London, 1992), RUTE-J (Andrews et al., 2006), Randoop (Pacheco & Ernst, 2007), Jtop (Zhang et al., 2009), Open-SourceRed (Kauffman and Kapfhammer, 2012)) so that desired alterations can be performed to adapt and enhance these tools according to the diverse set of requirments. While numerous other tools allow only limited (GZoltar (Campos et al., 2012), MINTS (Hsu & Orso, 2009), JINSI (Burger & Zeller, 2011)) or no customizability (TEMSA [34], Rostra [27], GenRed [31], USbRed [36], CPUT [37], EDTSO [41]). It is really hard to find the available support for numerous tools. A significant hurdle in this regard is language barrier as some research groups have their web sites and support in languages other than English (RTL (Woo et al., 2007), PLOOSE (Chae et al., 2011), TOBIAS (Dadeau et al., 2007)). This makes it hard to find the available support for respective tools. Moreover some proprietary academic research tools (USbRed (Sampath et al., 2007), CPUT (Sampath et al., 2011)) were only accessible through private search spaces, making it difficult to access executables and supporting materials. While considering evaluation of these tools, very few tools have been externally evaluated. This fact points to a significant research gap regarding empirical evaluation of existing TSR tools. Most of the freely available TSR tools are open source and we have provided download links for source codes and support for these tools (ATAC (Horgan & London, 1992), RUTE-J (Andrews et al., 2006), Randoop (Pacheco & Ernst, 2007), Jtop (Zhang et al., 2009), Open-SourceRed (Kauffman & Kapfhammer, 2012), MINTS (Hsu & Orso, 2009)). This aggregation of information will help researchers in acquiring useful information for empirical evaluation and extension of existing tools.

5. Open research issues
This section presents potential research issues related to automation support for TSR. The highlighted research issues will assist the researchers in improving limitations of existing TSR tools. Furthermore, it would help in proposing such tools that can efficiently meet the challenges of increasingly popular applications, including software product lines (Bosch, 2002), mobile applications (Zhang & Adipat, 2005), service-oriented architectures (Erl, 2005), web applications (Ali et al., 2007), and cloud-based applications (Orso & Rothermel, 2014).
5.1. Multi-objective TSR optimization support
Naturally, TSR problem is a multi-objective optimization problem focusing on various defined cost and effectiveness related challenges. This challenge becomes more complicated due to the involvement of many user-defined conflicting objectives and constraints for TSR optimization. Consequently, the tester needs to define a minimal threshold for cost-effectiveness measure, which would be finally compared with the obtained results. However, finding an optimal threshold depends on different factors, which varies from system to system. For example, in the case of web-based systems, some loss in FDE can be acceptable compared to safety critical systems, which requires 100% FDE due to high impact on human life. The proposed tool should be able to define an optimal threshold value based on the given factors. Extensive system profiling enables a tool to accurately estimate the desirable threshold. Current TSR tools (82%, 16/19) mainly focused on the single-objective optimization type such as to determine the minimal RS, which is impractical for a testing scenario containing multiple objectives and constraints for optimization (Harman, 2011). Conversely, researchers have to develop multi-objective supported tools that can efficiently meet the testing challenges of future applications.
5.2. Hyper-heuristic software testing
The “hyper-heuristic software testing” opens a new way for TSR, which involves combining different regression testing activities such as reduction, selection, and prioritization, and employ various types of search algorithms such as Genetic Algorithm and Hill Climbing to generate a best-possible TSR optimization. Therefore, a TSR tool can utilize the hyper-heuristic software testing opportunity to produce an optimal solution by focusing on multiple regression activities rather than applying single activity. For example, the TSR tool can first determine the RS (i.e. reduction) and then finds the high-value (i.e. prioritization) test cases to get the right execution order of reduced tests (Khan, Rehman, & Malik, 2009). The tool can employ search algorithms to perform both reduction and prioritization. Similarly, the other possible scenarios (e.g., reduction and then prioritization, and prioritization and then reduction) can also be supported by the proposed tools.
5.3. Multiple server-based TSR optimization
The use of multiple servers is beneficial in improving the computational time, especially for large complex systems having an enormous pool of test cases (Sprenkle, Sampath, Gibson, Pollock, & Souter, 2005). So, in this situation, the tester can achieve efficient computational time at the cost of extra required servers. This could be feasible in a situation, where divide-and-conquer strategy is acceptable. For instance, first the optimization problem is divided into many sub-problems, and then each sub-problem is independently solved using a single server. Thus, the proposed tool can act as a controller to manage multiple servers. It first determines the sub-problems and assigns each sub-problem to the available server for optimization. Next, the tool collects the computed solutions from each server and merges into a single solution. After that, the tool removes the duplication from the obtained solutions (from multiple servers) to generate a final optimal solution. However, resource affordability plays a vital role in the adoption of concurrent processing. This could be possible if low-cost servers are available or the tester has sufficient computational resources. One feasible solution is to employ Cloud Computing (Shiraz, Gani, Shamim, Khan, & Ahmad, 2015) for achieving low-cost computational resources without compromising the quality of the obtained optimized solution.
5.4. Automated evaluation of solution quality
Researchers can determine efficacy of obtained test reduction solution based on the proposed measures of cost (i.e. loss in FDE and tool execution time) and effectiveness (i.e. size of reduced suite and achieved coverage of the SUT). There may be a high possibility of miscalculations in determining the efficacy of an optimized solution due to human involvement in the test result's evaluation process. Since, cost-effectiveness measures are well-defined; they can be easily implemented and embedded into the TSR tool. Consequently, it helps to generate the RS along with associated statistical analysis (using defined cost-effectiveness measures) that provides confidence to the tester about the obtained solution. Similarly, the statistical report can assist the tester in the decision-making process by providing a multi-facet view of the targeted TSR optimization problem.

5.5. Hybrid solution and agile software development
Hybrid solutions for optimization problems show significant improvements in existing results by combining the strong points of two different TSR approaches (Yoo & Harman, 2010). Future tools can use the strengths of existing TSR approaches such as coverage-based and search-based to find a more effective solution for TSR problem (Harman & Mcminn, 2010). For example, the future tool first selects a set of diverse test cases by employing a search-based approach. Next, the adequacy of computed solution in terms of attained coverage is computed by using the coverage-based approach. Consequently, the integration of search-based and coverage-based approaches helps to provide an optimal solution of TSR problem with respect to high coverage of SUT. Notice that achieved coverage of the computed solution acts as a quality indicator, which is beneficial in deciding whether the test optimization process should be continued or stopped.
The TSR tool is highly beneficial to support agile software-development process (Stober & Hansmann, 2010). One of the important practices of agile software development is on the notion “testing early and testing often”. In this scenario, test cases signify instant feedback to the tester, whether the tested portion of a program is free of error or introduces new regression errors. Such tool would be also practical for web application testing using online mode of computation.]]></section>
    <section name="Conclusion and future remarks" category="conclusion">
        
6. Conclusion and future remarks
<h target="1" match="part">Test Suite Reduction (TSR) remains a predominant research area for past two decades, which has resulted in various types of TSR tools supporting different testing domains.</h><h target="4" match="part">This paper has critically evaluated current state-of-the-art TSR frameworks and tools. Furthermore, we provided a global view on automation support perspective of TSR by analyzing the reported tools based on a set of comparison criteria.</h><h target="2" match="part"> A thematic taxonomy was devised to classify the existing literature. </h><h target="5" match="part">Moreover, several potential research issues are presented that need further research to develop cost-effective TSR tools.</h>

 <![CDATA[Current state-of-the-art TSR frameworks and tools have mainly focused on coverage-based approaches (74%, 14/19) that exploit the coverage of a system under test to determine the reduced suite. However, achieving high coverage of a software system seldom guarantees to detect all possible faults. Alternatively, search-based approaches have the better potential to expose real faults by finding the diversity among prescribed test cases. Conversely, Integer Linear Programming-based approaches ensure Pareto-optimal solution for TSR. In contrast, similarity-based approaches generate a desired number of test cases having minimal similarity without considering coverage requirements of the system under test. On the other hand, existing TSR tools mainly targeted to solve the single-objective TSR optimization problems (82%, 16/19), which is impractical for a testing scenario containing multiple objectives and constraints. Therefore, it is crucial to pay more attention on multi-objective optimization problems in order to find better cost-effective solutions according to the nature of the application under test. Furthermore, existing TSR tools determine the reduced test suite in a sequential manner. Nevertheless, concurrent computation using multiple resources augments the desired computational time.

The consideration of multi-objective optimization, hyper-heuristic software testing, concurrent processing, automated result evaluation, agile development support, and hybrid solutions can augment the capabilities of existing TSR tools. Recently, similarity-based approaches are gaining high attention of researchers to provide an optimal solution to TSR problem. Hence, future research should focus on devising similarity-based automation support for TSR. In future, we plan to develop a multi-server based TSR tool to efficiently solve the multi-objective TSR optimization problems.]]></section>
</publication>